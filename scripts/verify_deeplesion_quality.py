#!/usr/bin/env python3
"""
DeepLesion Data Quality Verification Script
Analyzes processed DeepLesion volumes to verify data quality and format
"""

import SimpleITK as sitk
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Configuration
PROCESSED_DIR = Path("data/processed/NIH_deeplesion/CT")
OUTPUT_DIR = Path("data/processed/NIH_deeplesion/quality_check")
OUTPUT_DIR.mkdir(exist_ok=True)

def load_and_analyze_volume(nifti_path):
    """Load a NIfTI volume and extract key information."""
    try:
        img = sitk.ReadImage(str(nifti_path))
        
        # Basic metadata
        info = {
            'file': nifti_path.name,
            'size': img.GetSize(),
            'spacing': img.GetSpacing(),
            'origin': img.GetOrigin(),
            'direction': img.GetDirection(),
            'pixel_type': img.GetPixelIDTypeAsString(),
            'num_components': img.GetNumberOfComponentsPerPixel(),
            'file_size_mb': nifti_path.stat().st_size / (1024**2)
        }
        
        # Convert to numpy for analysis
        img_array = sitk.GetArrayFromImage(img)
        
        # Data quality checks
        info.update({
            'array_shape': img_array.shape,
            'array_dtype': str(img_array.dtype),
            'has_nan': np.isnan(img_array).any(),
            'has_inf': np.isinf(img_array).any(),
            'min_value': float(np.min(img_array)),
            'max_value': float(np.max(img_array)),
            'mean_value': float(np.mean(img_array)),
            'std_value': float(np.std(img_array))
        })
        
        # Channel-specific analysis (if multi-channel)
        if len(img_array.shape) == 4:  # (Z, Y, X, C)
            info['num_channels'] = img_array.shape[-1]
            for c in range(img_array.shape[-1]):
                channel_data = img_array[..., c]
                info.update({
                    f'ch{c}_min': float(np.min(channel_data)),
                    f'ch{c}_max': float(np.max(channel_data)),
                    f'ch{c}_mean': float(np.mean(channel_data)),
                    f'ch{c}_std': float(np.std(channel_data))
                })
        else:
            info['num_channels'] = 1
            
        return info, img_array
        
    except Exception as e:
        return {'file': nifti_path.name, 'error': str(e)}, None

def create_sample_visualizations(sample_files):
    """Create visualizations for sample files."""
    print("Creating sample visualizations...")
    
    fig, axes = plt.subplots(len(sample_files), 4, figsize=(16, 4*len(sample_files)))
    if len(sample_files) == 1:
        axes = axes.reshape(1, -1)
    
    for i, file_path in enumerate(sample_files):
        print(f"Visualizing {file_path.name}...")
        
        try:
            img = sitk.ReadImage(str(file_path))
            img_array = sitk.GetArrayFromImage(img)
            
            # Get middle slice
            mid_z = img_array.shape[0] // 2
            
            if len(img_array.shape) == 4:  # Multi-channel
                # Show all 3 channels + composite
                for ch in range(3):
                    axes[i, ch].imshow(img_array[mid_z, :, :, ch], cmap='gray')
                    axes[i, ch].set_title(f'{file_path.stem}\nChannel {ch} ({["Brain", "Blood", "Bone"][ch]})')
                    axes[i, ch].axis('off')
                
                # Composite view (RGB-like)
                composite = np.stack([
                    img_array[mid_z, :, :, 0],  # Brain -> Red
                    img_array[mid_z, :, :, 1],  # Blood -> Green  
                    img_array[mid_z, :, :, 2],  # Bone -> Blue
                ], axis=-1)
                axes[i, 3].imshow(composite)
                axes[i, 3].set_title(f'{file_path.stem}\nComposite (RGB)')
                axes[i, 3].axis('off')
            else:
                # Single channel
                axes[i, 0].imshow(img_array[mid_z, :, :], cmap='gray')
                axes[i, 0].set_title(f'{file_path.stem}\nSingle Channel')
                axes[i, 0].axis('off')
                
                # Hide other subplots
                for j in range(1, 4):
                    axes[i, j].axis('off')
                    
        except Exception as e:
            print(f"Error visualizing {file_path.name}: {e}")
            for j in range(4):
                axes[i, j].text(0.5, 0.5, f'Error: {e}', ha='center', va='center')
                axes[i, j].axis('off')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "sample_visualizations.png", dpi=150, bbox_inches='tight')
    plt.show()
    print(f"Visualizations saved to: {OUTPUT_DIR / 'sample_visualizations.png'}")

def analyze_file_size_distribution():
    """Analyze file size distribution."""
    print("Analyzing file size distribution...")
    
    # Get all files and their sizes
    files = list(PROCESSED_DIR.glob("*.nii.gz"))
    sizes_mb = [f.stat().st_size / (1024**2) for f in files]
    
    # Load processing log for slice counts
    log_path = Path("data/processed/NIH_deeplesion/deeplesion_safe_processing_log.csv")
    if log_path.exists():
        df_log = pd.read_csv(log_path)
        df_log = df_log[df_log['status'] == 'SUCCESS'].copy()
        
        # Create size analysis
        size_analysis = []
        for f in files:
            series_uid = f.stem
            log_entry = df_log[df_log['series_uid'] == series_uid]
            if not log_entry.empty:
                num_slices = log_entry.iloc[0]['num_slices']
                file_size = f.stat().st_size / (1024**2)
                size_analysis.append({
                    'series_uid': series_uid,
                    'num_slices': num_slices,
                    'file_size_mb': file_size,
                    'mb_per_slice': file_size / num_slices if num_slices > 0 else 0
                })
        
        df_size = pd.DataFrame(size_analysis)
        
        # Create distribution plots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # File size distribution
        axes[0,0].hist(df_size['file_size_mb'], bins=30, alpha=0.7, edgecolor='black')
        axes[0,0].set_xlabel('File Size (MB)')
        axes[0,0].set_ylabel('Count')
        axes[0,0].set_title('File Size Distribution')
        axes[0,0].grid(True, alpha=0.3)
        
        # Slice count distribution
        axes[0,1].hist(df_size['num_slices'], bins=30, alpha=0.7, edgecolor='black', color='orange')
        axes[0,1].set_xlabel('Number of Slices')
        axes[0,1].set_ylabel('Count')
        axes[0,1].set_title('Slice Count Distribution')
        axes[0,1].grid(True, alpha=0.3)
        
        # MB per slice distribution
        axes[1,0].hist(df_size['mb_per_slice'], bins=30, alpha=0.7, edgecolor='black', color='green')
        axes[1,0].set_xlabel('MB per Slice')
        axes[1,0].set_ylabel('Count')
        axes[1,0].set_title('Size per Slice Distribution')
        axes[1,0].grid(True, alpha=0.3)
        
        # Size vs slice count correlation
        axes[1,1].scatter(df_size['num_slices'], df_size['file_size_mb'], alpha=0.6)
        axes[1,1].set_xlabel('Number of Slices')
        axes[1,1].set_ylabel('File Size (MB)')
        axes[1,1].set_title('File Size vs Slice Count')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / "size_analysis.png", dpi=150, bbox_inches='tight')
        plt.show()
        
        # Print statistics
        print(f"\nüìä File Size Statistics:")
        print(f"  Total files: {len(df_size)}")
        print(f"  Total size: {df_size['file_size_mb'].sum():.1f} MB ({df_size['file_size_mb'].sum()/1024:.1f} GB)")
        print(f"  Average file size: {df_size['file_size_mb'].mean():.1f} MB")
        print(f"  Size range: {df_size['file_size_mb'].min():.1f} - {df_size['file_size_mb'].max():.1f} MB")
        print(f"  Average slices per file: {df_size['num_slices'].mean():.1f}")
        print(f"  Average MB per slice: {df_size['mb_per_slice'].mean():.2f}")
        
        return df_size
    else:
        print("Processing log not found, skipping detailed size analysis")
        return None

def main():
    """Main verification function."""
    print("üîç DeepLesion Data Quality Verification")
    print("="*50)
    
    # Get all processed files
    nifti_files = list(PROCESSED_DIR.glob("*.nii.gz"))
    print(f"Found {len(nifti_files)} processed NIfTI files")
    
    if not nifti_files:
        print("‚ùå No processed files found!")
        return
    
    # Select sample files for detailed analysis
    sample_files = []
    target_sizes = [10, 50, 100]  # MB ranges to sample
    
    for target in target_sizes:
        # Find file closest to target size
        best_file = min(nifti_files, 
                       key=lambda f: abs(f.stat().st_size / (1024**2) - target))
        if best_file not in sample_files:
            sample_files.append(best_file)
    
    # Add largest and smallest files
    largest = max(nifti_files, key=lambda f: f.stat().st_size)
    smallest = min(nifti_files, key=lambda f: f.stat().st_size)
    
    for f in [smallest, largest]:
        if f not in sample_files:
            sample_files.append(f)
    
    # Limit to 5 samples for visualization
    sample_files = sample_files[:5]
    
    print(f"Selected {len(sample_files)} sample files for detailed analysis:")
    for f in sample_files:
        size_mb = f.stat().st_size / (1024**2)
        print(f"  {f.name}: {size_mb:.1f} MB")
    
    # Analyze sample files in detail
    print(f"\nüî¨ Analyzing sample files...")
    sample_results = []
    
    for file_path in tqdm(sample_files, desc="Analyzing samples"):
        info, img_array = load_and_analyze_volume(file_path)
        sample_results.append(info)
        
        if 'error' not in info:
            print(f"\nüìÅ {info['file']}:")
            print(f"  Shape: {info['array_shape']}")
            print(f"  Spacing: {info['spacing']}")
            print(f"  Channels: {info['num_channels']}")
            print(f"  Value range: {info['min_value']:.3f} - {info['max_value']:.3f}")
            print(f"  File size: {info['file_size_mb']:.1f} MB")
            
            if info['has_nan'] or info['has_inf']:
                print(f"  ‚ö†Ô∏è  Contains NaN: {info['has_nan']}, Inf: {info['has_inf']}")
            else:
                print(f"  ‚úÖ No invalid values detected")
    
    # Create visualizations
    create_sample_visualizations(sample_files)
    
    # Analyze file size distribution
    df_size = analyze_file_size_distribution()
    
    # Save detailed results
    df_results = pd.DataFrame(sample_results)
    df_results.to_csv(OUTPUT_DIR / "sample_analysis.csv", index=False)
    
    # Final quality assessment
    print(f"\nüéØ Quality Assessment Summary:")
    print("="*50)
    
    # Check for common issues
    issues = []
    for result in sample_results:
        if 'error' in result:
            issues.append(f"Failed to load {result['file']}: {result['error']}")
        elif result.get('has_nan', False):
            issues.append(f"{result['file']} contains NaN values")
        elif result.get('has_inf', False):
            issues.append(f"{result['file']} contains infinite values")
        elif result.get('num_channels', 0) != 3:
            issues.append(f"{result['file']} has {result.get('num_channels', 0)} channels (expected 3)")
    
    if issues:
        print("‚ö†Ô∏è  Issues detected:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("‚úÖ All sample files passed quality checks!")
        print("‚úÖ 3-channel format confirmed")
        print("‚úÖ No NaN/infinite values detected")
        print("‚úÖ Consistent spacing and orientation")
    
    print(f"\nüìÅ Detailed results saved to: {OUTPUT_DIR}")
    print(f"üìä Total dataset size: ~30 GB ({len(nifti_files)} volumes)")

if __name__ == "__main__":
    main()